--Analysis of Coverage for Two New Random Card Tests--

	The coverage for the two new random tests of the Adventurer card and the Cutpurse card are printed at
the bottom of their respective .out files.  I have also included it here:

Coverage for randomtestadventurer:

Lines executed:21.66% of 577
Branches executed:19.66% of 417
Taken at least once:17.51% of 417
Calls executed:10.53% of 95

Coverage for randomtestcard (Cutpurse):

Lines executed:19.93% of 577
Branches executed:19.66% of 417
Taken at least once:16.55% of 417
Calls executed:10.53% of 95

	Individually, they each cover about 20% of the code.  But some of the code they cover may be the same,
so below is the coverage for both tests at once.

Coverage of randomtestadventurer and randomtestcard Combined:

Lines executed:26.00% of 577
Branches executed:23.50% of 417
Taken at least once:21.10% of 417
Calls executed:13.68% of 95

	So the two test together cover 26% of the code.  Clearly many of the functions called overlap, since
the combined coverage is nowhere near the sum of the two tests coverage (which would be about 40%).  The
Next question to consider is how much these tests have improved the coverage compared to the set of
unittests and cardtests that were made for the previous assignment.  Here is the total coverage for 
those eight tests:

Lines executed:29.81% of 577
Branches executed:34.29% of 417
Taken at least once:27.34% of 417
Calls executed:16.84% of 95

	So even though they are not random tests, they do cover more code than simply two random tests.  Let
us look at how much code coverage there is for all the tests--the eight old and the two new.

Combined coverage of unittests, cardtests, and the two random tests:

Lines executed:39.17% of 577
Branches executed:44.84% of 417
Taken at least once:36.45% of 417
Calls executed:22.11% of 95
dominion.c:creating 'dominion.c.gcov'

	Certainly the random tests increased the code coverage.  Going from about 29% to 39% looks quite 
impressive--especially considering that both these cards were included in the original cardtests.  So 
one might conclude that randomization accounted for this large increase in coverage.  However, I do not 
think it is the main factor; I think the most significant reason for the coverage increase is that I am 
now using the initializeGame() function at the start of my tests.  This function likely calls many other
functions as it has to set up the game.  The reason I wanted to start using the initializeGame() function 
is that on Piazza it was said that it prevented the test program for the adventurer card from crashing 
when deck and discard were both zero.  Since I didn't want my code to crash in the middle of a bunch of
tests when it actually was not a bug, I switched to using initalizeGame().

	I would like to point something out about the design of my random tests--as it does not come out in
the code coverage.  I made sure that boundary cases would be covered more often than other random inputs.
For both these tests, I have a randCount() function that will randomly select one of three options: the
minimum boundary, the minimum boundary plus 1, and anything from the minimum to the maximum (which in 
the case of deck and hand counts was up to 500!).  I considered doing the maximum as a boundary case, but
in the Dominion game, it is very unlikely anyone would have a deck (or hand!) of 500, but it is quite
reasonable that they would have a deck of 0 or 1.  This increase in boundary checking is particularly 
important since several different counts (deck, hand, discard, hand position, etc) are being
randomized at once.  Without my bias, it is far less likely that something like having both the 
deck and discard be zero at the same time.  Of course, with enough random tests this should happen
eventually, but I felt it would be more useful to spend more time running cases that are likely
to produce failures.
